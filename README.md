# Bearing Fault Detection with SHAP Explainability

A comprehensive machine learning project implementing **SHAP (SHapley Additive exPlanations)** for explainable bearing fault classification using the Case Western Reserve University (CWRU) Bearing Dataset. This project demonstrates how explainable AI can bridge machine learning models with mechanical engineering practice, providing transparent and actionable insights for predictive maintenance.

## üìã Table of Contents

- [Overview](#overview)
- [Dataset](#dataset)
- [SHAP Implementation](#shap-implementation)
- [Methodology](#methodology)
- [Key Results](#key-results)
- [Key Findings](#key-findings)
- [Features Analyzed](#features-analyzed)
- [Model Performance](#model-performance)
- [SHAP Insights](#shap-insights)
- [Installation & Usage](#installation--usage)
- [Project Structure](#project-structure)
- [Contributions](#contributions)

## üéØ Overview

This project implements and analyzes bearing fault detection models using **Support Vector Classifier (SVC)** and **Logistic Regression**, enhanced with comprehensive SHAP explainability analysis. The goal is to:

1. **Achieve high-accuracy fault classification** across 10 different bearing fault types
2. **Provide interpretable explanations** for model predictions using SHAP
3. **Identify critical features** for each fault type and severity level
4. **Enable actionable insights** for predictive maintenance strategies

**Key Achievement**: 96.4% test accuracy (723/750 correct predictions) with 27 misclassifications on the full test set, demonstrating state-of-the-art performance with full interpretability. Detailed SHAP analysis was performed on a 200-sample subset achieving 98% accuracy (196/200 correct).

## üìä Dataset

### CWRU Bearing Dataset

The **Case Western Reserve University Bearing Dataset** contains vibration signals from bearings with artificially introduced defects. This dataset is widely used in predictive maintenance research and industrial diagnostics.

**Dataset Characteristics:**
- **Total Samples**: 2,300 samples across 10 fault classes
- **Sampling Frequency**: 48 kHz
- **Time Segment Length**: 2,048 points (0.04 seconds per segment)
- **Motor Load**: 1 HP
- **Shaft Speed**: 1,772 RPM

### Fault Types

The dataset includes **10 distinct fault conditions**:

| Fault Type | Description | Defect Size |
|------------|-------------|-------------|
| **Normal_1** | Healthy bearing (baseline) | - |
| **Ball_007_1** | Ball defect | 0.007" (0.178 mm) |
| **Ball_014_1** | Ball defect | 0.014" (0.356 mm) |
| **Ball_021_1** | Ball defect | 0.021" (0.533 mm) |
| **IR_007_1** | Inner Race fault | 0.007" (0.178 mm) |
| **IR_014_1** | Inner Race fault | 0.014" (0.356 mm) |
| **IR_021_1** | Inner Race fault | 0.021" (0.533 mm) |
| **OR_007_6_1** | Outer Race fault (6 o'clock) | 0.007" (0.178 mm) |
| **OR_014_6_1** | Outer Race fault (6 o'clock) | 0.014" (0.356 mm) |
| **OR_021_6_1** | Outer Race fault (6 o'clock) | 0.021" (0.533 mm) |

### Features

Nine statistical features are extracted from time-domain vibration signals:

1. **Maximum** - Peak amplitude value
2. **Minimum** - Minimum amplitude value
3. **Mean** - Average amplitude
4. **Standard Deviation (SD)** - Amplitude variability
5. **RMS** - Root Mean Square (energy measure)
6. **Skewness** - Distribution asymmetry
7. **Kurtosis** - Distribution tail heaviness
8. **Crest Factor** - Peak-to-RMS ratio
9. **Form Factor** - RMS-to-mean ratio

## üîç SHAP Implementation

### What is SHAP?

**SHAP (SHapley Additive exPlanations)** is a unified framework for explaining machine learning model outputs. It provides:

- **Global Explanations**: Overall feature importance across all predictions
- **Local Explanations**: Individual prediction explanations showing why a specific sample was classified as a particular fault
- **Feature Interactions**: How features work together to influence predictions

### SHAP Explainers Used

1. **KernelExplainer** (for SVC)
   - Model-agnostic explainer suitable for non-linear models
   - Uses weighted linear regression to approximate SHAP values
   - Handles multi-class classification with probability outputs

2. **LinearExplainer** (for Logistic Regression)
   - Optimized for linear models
   - Computationally efficient
   - Provides exact SHAP values for linear relationships

### SHAP Visualizations Implemented

1. **Summary Plots (Beeswarm)**
   - Shows distribution of SHAP values for each feature
   - Color-coded by feature value (red = high, blue = low)
   - Reveals feature impact patterns

2. **Bar Plots**
   - Mean absolute SHAP values per feature
   - Quick overview of global feature importance
   - Easy to interpret ranking

3. **Waterfall Plots**
   - Individual prediction explanations
   - Shows how each feature pushes prediction from base value
   - Critical for understanding specific misclassifications

4. **Partial Dependence Plots**
   - Shows how individual features affect predictions
   - Reveals non-linear relationships
   - Guides feature engineering

5. **Feature Importance Heatmaps**
   - Fault-type-specific feature importance
   - Severity progression patterns
   - Visual comparison across fault types

## üî¨ Methodology

### 1. Data Preprocessing

- **Train-Test Split**: 67.4% training (1,550 samples), 32.6% testing (750 samples)
- **Feature Scaling**: StandardScaler for normalization
- **Class Balancing**: Perfectly balanced dataset (230 samples per class in original dataset; 75 samples per class in test set after split)

### 2. Model Training

**Support Vector Classifier (SVC)**
- Hyperparameter tuning via GridSearchCV (10-fold cross-validation)
- Kernel: RBF (Radial Basis Function)
- Probability estimation enabled for SHAP compatibility
- Best model selected based on cross-validation performance

**Logistic Regression**
- Multinomial classification
- L2 regularization
- Baseline linear model for comparison

### 3. SHAP Analysis Pipeline

```
1. Model Training ‚Üí 2. SHAP Explainer Initialization ‚Üí 3. SHAP Value Calculation
         ‚Üì                          ‚Üì                              ‚Üì
    Best Models              Kernel/Linear Explainer         Sample Selection
         ‚Üì                          ‚Üì                              ‚Üì
4. Global Analysis ‚Üí 5. Fault-Specific Analysis ‚Üí 6. Misclassification Analysis
         ‚Üì                          ‚Üì                              ‚Üì
   Feature Ranking          Severity Patterns              Error Explanations
```

### 4. Evaluation Metrics

- **Accuracy**: Overall classification correctness
- **Precision**: Per-class precision scores
- **Recall**: Per-class recall scores
- **F1-Score**: Harmonic mean of precision and recall
- **ROC-AUC**: Area under ROC curve (multi-class)
- **PR-AUC**: Area under Precision-Recall curve

## üìà Key Results

### Model Performance Summary

| Model | Accuracy | Avg ROC-AUC | Avg PR-AUC | Misclassifications |
|-------|----------|-------------|------------|-------------------|
| **SVC (Best)** | **96.4%** | **0.731** | **0.623** | **27/750** |
| **Logistic Regression** | **94.3%** | **0.656** | **0.606** | **43/750** |

**Note**: Performance metrics are reported on the full test set (750 samples). Detailed SHAP explainability analysis was performed on a 200-sample subset for computational efficiency, achieving 98% accuracy (196/200 correct) on that subset.

### Per-Class Performance (SVC)

- **Perfect Classification** (100% precision/recall on initial model): IR_007_1, IR_014_1, IR_021_1, OR_007_6_1, OR_021_6_1, Normal_1
- **High Performance** (>95% F1-score): Ball_007_1, Ball_021_1
- **Good Performance** (>90% F1-score): Ball_014_1
- **Note**: Some classes (Ball_014_1, Ball_021_1, OR_014_6_1) showed multiple misclassifications in the full test set, primarily involving confusion between similar severity levels

### Misclassification Analysis

**Full Test Set (750 samples)**: **27 misclassifications** (3.6% error rate)

**Primary Misclassification Patterns:**
- Ball_014_1: 9 misclassifications (confused with Normal_1, OR_014_6_1, and other classes)
- Ball_021_1: 8 misclassifications (primarily confused with OR_014_6_1)
- OR_014_6_1: 8 misclassifications (confused with Ball_021_1, Ball_014_1, and Ball_007_1)
- Ball_007_1: 1 misclassification
- OR_021_6_1: 1 misclassification

**SHAP Analysis Subset (200 samples)**: **4 misclassifications** analyzed in detail:
1. OR_014_6_1 ‚Üí Predicted as Ball_021_1
2. Ball_014_1 ‚Üí Predicted as Normal_1
3. Ball_021_1 ‚Üí Predicted as OR_014_6_1

**Key Insight**: Most errors involve confusion between similar severity levels (0.014" vs 0.021") and between Ball and Outer Race faults at similar severities, indicating the model struggles with subtle severity distinctions but excels at fault location identification for clearly distinct classes.

## üîë Key Findings

### 1. Global Feature Importance

**SVC Model Top Features:**
1. **Crest Factor** (0.0508) - Most discriminative
2. **Minimum** (0.0366) - Extreme value detection
3. **Mean** (0.0291) - Amplitude baseline
4. **Skewness** (0.0256) - Distribution shape
5. **Kurtosis** (0.0222) - Tail behavior

**Logistic Regression Top Features:**
1. **Mean** (1.739) - Dominant feature
2. **Standard Deviation** (1.462)
3. **RMS** (1.456)
4. **Form Factor** (1.200)
5. **Kurtosis** (1.142)

**Critical Insight**: Only **Mean** and **Kurtosis** appear in top 5 for both models, indicating these are universal fault indicators regardless of model architecture.

### 2. Fault-Location-Specific Patterns

**Inner Race (IR) Faults:**
- **Primary Indicator**: Mean amplitude (importance: 0.1299 for IR_014_1)
- **Pattern**: Mean importance decreases with severity (0.1299 ‚Üí 0.0828 ‚Üí 0.0300)
- **Implication**: Amplitude monitoring critical for IR fault detection

**Outer Race (OR) Faults:**
- **Primary Indicator**: Kurtosis (importance: 0.0805 for OR_021_6_1)
- **Pattern**: Kurtosis importance increases with severity (0.0310 ‚Üí 0.0470 ‚Üí 0.0805)
- **Implication**: Distribution shape analysis essential for OR faults

**Ball Faults:**
- **Primary Indicators**: Mean + Kurtosis (balanced approach)
- **Pattern**: Relatively stable feature patterns across severities
- **Implication**: Combined amplitude and shape analysis needed

**Normal Condition:**
- **Pattern**: Requires balanced consideration of multiple features
- **Top Features**: Kurtosis, RMS, SD, Mean (all similar importance)

### 3. Model Comparison Insights

**Feature Agreement:**
- **Common Features**: Only Mean and Kurtosis (limited overlap)
- **SVC-Specific**: Crest factor, Min, Skewness (non-linear patterns)
- **LR-Specific**: SD, Form Factor, RMS (linear amplitude relationships)

**Correlation Analysis:**
- **Feature Importance Correlation**: -0.404 (negative correlation)
- **Interpretation**: Models use fundamentally different decision strategies
- **SVC**: Emphasizes distribution shape and extremes
- **LR**: Emphasizes central tendency and variability

**Practical Implication**: Ensemble methods could leverage complementary strengths of both models.

### 4. Severity Progression Patterns

| Fault Type | Severity Trend | Key Feature |
|------------|----------------|-------------|
| **IR Faults** | Mean ‚Üì with severity | Mean amplitude |
| **OR Faults** | Kurtosis ‚Üë with severity | Distribution shape |
| **Ball Faults** | Stable patterns | Mean + Kurtosis |

**Predictive Maintenance Value**: These patterns enable tracking fault progression by monitoring specific feature trends.

## üìä SHAP Insights

### Global Explanations

**Feature Ranking (SVC):**
- Crest factor dominates, indicating peak-to-average ratio is critical
- Minimum values important for detecting extreme events
- Statistical distribution features (skewness, kurtosis) capture non-linear patterns

**Feature Ranking (Logistic Regression):**
- Mean amplitude is the dominant linear feature
- Standard deviation and RMS provide complementary amplitude information
- Form factor captures waveform characteristics

### Local Explanations

**Correctly Classified Samples:**
- Strong feature alignment with expected fault characteristics
- Clear separation in SHAP value distributions
- Consistent patterns across samples of same fault type

**Misclassified Samples:**
- Mixed feature signals with intermediate values
- Conflicting SHAP contributions
- Boundary cases where feature distributions overlap

### Fault-Specific Insights

**SHAP Analysis Reveals:**
1. Different fault locations require different monitoring strategies
2. Severity progression follows predictable feature trends
3. Feature importance varies significantly by fault type
4. Some features are universally important (mean, kurtosis)
5. Others are fault-specific (crest for SVC, SD for LR)

## üöÄ Installation & Usage

### Prerequisites

```bash
Python 3.8+
```

### Required Libraries

```bash
pip install numpy pandas scikit-learn matplotlib seaborn shap
```

Or install from requirements:

```bash
pip install -r requirements.txt
```

### Running the Analysis

1. **Open the Jupyter Notebook:**
   ```bash
   jupyter notebook crwu-bearings-benchmarks-shap-explainability.ipynb
   ```

2. **Execute Cells Sequentially:**
   - The notebook is organized into logical sections
   - Run all cells from top to bottom for complete analysis
   - SHAP visualizations require JavaScript support (handled automatically)

3. **Key Sections:**
   - **Data Loading**: Loads CWRU bearing dataset
   - **EDA**: Exploratory data analysis
   - **Model Training**: SVC and Logistic Regression
   - **SHAP Analysis**: Comprehensive explainability analysis
   - **Visualizations**: ROC curves, PR curves, confusion matrices
   - **Key Findings**: Detailed interpretation of results

### Expected Runtime

- **Model Training**: ~5-10 minutes (GridSearchCV optimization)
- **SHAP Value Calculation**: ~10-15 minutes (KernelExplainer for SVC)
- **Visualizations**: ~2-3 minutes
- **Total**: ~20-30 minutes for complete analysis

## üìÅ Project Structure

```
project/
‚îÇ
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ DATASET.md                         # Dataset documentation
‚îú‚îÄ‚îÄ crwu-bearings-benchmarks-shap-explainability.ipynb  # Main analysis notebook
‚îÇ
‚îú‚îÄ‚îÄ kaggle/
‚îÇ   ‚îî‚îÄ‚îÄ input/
‚îÇ       ‚îî‚îÄ‚îÄ cwru-bearing-datasets/
‚îÇ           ‚îú‚îÄ‚îÄ feature_time_48k_2048_load_1.csv      # Preprocessed features
‚îÇ           ‚îú‚îÄ‚îÄ CWRU_48k_load_1_CNN_data.npz          # NumPy data file
‚îÇ           ‚îî‚îÄ‚îÄ raw/                                  # Raw vibration signals
‚îÇ               ‚îú‚îÄ‚îÄ B007_1_123.mat                    # Ball fault samples
‚îÇ               ‚îú‚îÄ‚îÄ IR007_1_110.mat                   # Inner race samples
‚îÇ               ‚îú‚îÄ‚îÄ OR007_6_1_136.mat                 # Outer race samples
‚îÇ               ‚îî‚îÄ‚îÄ Time_Normal_1_098.mat             # Normal samples
‚îÇ
‚îî‚îÄ‚îÄ venv/                              # Virtual environment (optional)
```

## üéì Key Contributions

### Technical Contributions

1. **Comprehensive SHAP Implementation**
   - Both KernelExplainer and LinearExplainer
   - Multi-class classification support
   - 3D SHAP value handling for complex model outputs

2. **Fault-Specific Analysis**
   - Per-fault-type feature importance
   - Severity progression tracking
   - Location-specific monitoring strategies

3. **Model Comparison Framework**
   - SVC vs Logistic Regression comparison
   - Feature importance correlation analysis
   - Ensemble potential identification

4. **Misclassification Analysis**
   - SHAP-based error explanation
   - Boundary case identification
   - Improvement opportunity detection

### Practical Contributions

1. **Actionable Insights for Industry**
   - Sensor selection guidance
   - Monitoring strategy recommendations
   - Feature prioritization for different fault types

2. **Predictive Maintenance Framework**
   - Severity progression patterns
   - Early warning indicators
   - Feature trend tracking

3. **Interpretability Best Practices**
   - Global + local explanations
   - Visual communication of complex models
   - Trust-building through transparency

## üìö References

- **SHAP Paper**: Lundberg, S. M., & Lee, S. I. (2017). "A unified approach to interpreting model predictions." NIPS.
- **CWRU Dataset**: Case Western Reserve University Bearing Data Center
- **Scikit-learn**: Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.

## üîÆ Future Work

1. **Feature Engineering**
   - Frequency-domain features for improved severity distinction
   - Interaction terms between top SHAP features
   - Envelope analysis for early fault detection

2. **Model Enhancement**
   - Ensemble methods combining SVC + LR
   - Uncertainty quantification for borderline cases
   - Fault severity regression models

3. **Deployment**
   - Real-time SHAP dashboard
   - Automatic alert rules based on feature contributions
   - Operator training materials using SHAP explanations

4. **Extension**
   - Additional bearing datasets
   - Other rotating machinery (gears, motors, pumps)
   - Multi-sensor fusion with SHAP

## üìù License

This project uses the CWRU Bearing Dataset, which is publicly available for research purposes. Please refer to the dataset documentation for usage terms.

## üë• Acknowledgments

- **Case Western Reserve University** for providing the bearing dataset
- **SHAP developers** for the excellent explainability framework
- **Scikit-learn** team for comprehensive ML tools

---

**Note**: This project demonstrates that machine learning for industrial diagnostics can be both highly accurate (96.4% on full test set) and fully interpretable through SHAP. The insights gained can directly improve bearing monitoring systems, reduce unplanned downtime, and enhance predictive maintenance strategies.

For detailed analysis and visualizations, please refer to the Jupyter notebook: `crwu-bearings-benchmarks-shap-explainability.ipynb`

